{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run algebra.py\n",
    "%run cache.py\n",
    "%run costs.py\n",
    "%run features.py\n",
    "%run gradients.py\n",
    "%run helpers.py\n",
    "%run model.py\n",
    "%run models.py\n",
    "%run splits.py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB_SAMPLE = False\n",
    "CACHE_DIR = \"test/cache/\" if SUB_SAMPLE else \"cache/\"\n",
    "SUBMISSIONS_DIR = \"test/submissions/\" if SUB_SAMPLE else \"submissions/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x, ids = load_csv_data('data/train.csv', SUB_SAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Only Using Clean Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression with Fixed Degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Without Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression_MSE_FixedDegree_Model(Model):\n",
    "\n",
    "    def prepare(self, x, y, h):\n",
    "        \n",
    "        degree = int(h['degree'])\n",
    "\n",
    "        x = remove_errors(x)\n",
    "        x = remove_outliers(x)\n",
    "        x = standardize_all(x)\n",
    "        x = remove_nan_features(x)\n",
    "        x = build_poly(x, degree)\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "    def fit(self, x, y, h):\n",
    "\n",
    "        lambda_ = float(h['lambda'])\n",
    "        degree = int(h['degree'])\n",
    "\n",
    "        return ridge_regression(y, x, lambda_)     \n",
    "        \n",
    "    def test(self, x, y, w, h):\n",
    "                \n",
    "        return { 'mse': compute_mse(y, x, w) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = RidgeRegression_MSE_FixedDegree_Model()\n",
    "\n",
    "hs = { \n",
    "    'degree': np.arange(4, 16), \n",
    "    'lambda': np.logspace(-8, -2, 7),\n",
    "}\n",
    "\n",
    "res = myModel.evaluate(x, y, hs,filename=CACHE_DIR+'RidgeRegression_MSE_FixedDegree')\n",
    "res_mse = np.vectorize(lambda x: x['mse'])(res)\n",
    "\n",
    "plot_heatmap(res, hs, 'mse', 'degree', 'lambda')\n",
    "find_arg_min(res, 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Cross-Validation\n",
    "\n",
    "Here, we implement the same model with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = CrossValidationModel(RidgeRegression_MSE_FixedDegree_Model())\n",
    "\n",
    "hs = { \n",
    "    'degree': np.arange(4, 16), \n",
    "    'lambda': np.logspace(-8, -2, 7),\n",
    "    'k_fold': 4,\n",
    "    'seed': 0\n",
    "}\n",
    "\n",
    "res = myModel.evaluate(x, y, hs, CACHE_DIR+'RidgeRegression_MSE_FixedDegree_CrossValidation')\n",
    "\n",
    "plot_heatmap(res, hs, 'avg_mse_te', 'degree', 'lambda')\n",
    "best_h = find_arg_min(res, 'avg_mse_te')\n",
    "best_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel.predict(best_h, x, y, SUBMISSIONS_DIR + 'RidgeRegression_MSE_FixedDegree_CrossValidation_Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD_Lasso_MSE_FixedDegree_CrossValidation_Model(Model):\n",
    "\n",
    "    def prepare(self, x, y):\n",
    "\n",
    "        return clean_data(x), y\n",
    "\n",
    "    def fit(self, x, y, h={}):\n",
    "        \n",
    "        degree = int(h['degree'])\n",
    "        lambda_ = float(h['lambda'])\n",
    "        k_fold = int(h['k_fold'])\n",
    "        seed = int(h['seed'])\n",
    "        precision = float(h['precision'])\n",
    "        gamma = float(h['gamma'])\n",
    "        \n",
    "        batch_size = 1\n",
    "\n",
    "        avg_mse_tr = 0\n",
    "        avg_mse_te = 0\n",
    "\n",
    "        # split data in k fold\n",
    "        k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "        for k in range(0, k_fold):\n",
    "            \n",
    "            # get split data\n",
    "            x_tr, x_te, y_tr, y_te = cross_data(y, x, k_indices, k)\n",
    "\n",
    "            # form data with polynomial degree:\n",
    "            x_tr = build_poly(x_tr, degree)\n",
    "            x_te = build_poly(x_te, degree)\n",
    "            \n",
    "            w = np.zeros(x_tr.shape[1])\n",
    "            loss = float(\"inf\")\n",
    "            diff = float(\"inf\")\n",
    "\n",
    "            while diff > precision:\n",
    "                for y_batch, tx_batch in batch_iter(y_tr, x_tr, batch_size=batch_size, num_batches=1):\n",
    "                    \n",
    "                    # compute a stochastic gradient and loss\n",
    "                    err = y_batch - tx_batch.dot(w)\n",
    "                    grad = -tx_batch.T.dot(err) / len(err)\n",
    "                    \n",
    "                    # compute lasso\n",
    "                    omega = np.vectorize(lambda wi: -np.sign(wi))(w)          \n",
    "            \n",
    "                    # update w through the stochastic gradient update\n",
    "                    w = w - gamma * (grad + lambda_ * omega)\n",
    "                    \n",
    "                    loss += err * batch_size / y_tr.shape[0] \n",
    "\n",
    "                # calculate loss & update diff\n",
    "                diff = loss\n",
    "                loss = compute_mse(y_tr, x_tr, w)\n",
    "                diff = diff - loss\n",
    "                \n",
    "            # calculate the loss for train and test data + add it:\n",
    "            avg_mse_tr += compute_mse(y_tr, x_tr, w) / k_fold\n",
    "            avg_mse_te += compute_mse(y_te, x_te, w) / k_fold\n",
    "\n",
    "        return {\n",
    "            \"avg_mse_tr\": avg_mse_tr,\n",
    "            \"avg_mse_te\": avg_mse_te\n",
    "        }, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression_MSE_FixedDegree_Model(Model):\n",
    "\n",
    "    def prepare(self, x, y, h):\n",
    "        \n",
    "        degree = int(h['degree'])\n",
    "\n",
    "        x = remove_errors(x)\n",
    "        x = remove_outliers(x)\n",
    "        x = standardize_all(x)\n",
    "        x = remove_nan_features(x)\n",
    "        x = build_poly(x, degree)\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "    def fit(self, x, y, h):\n",
    "\n",
    "        lambda_ = float(h['lambda'])\n",
    "        degree = int(h['degree'])\n",
    "\n",
    "        return ridge_regression(y, x, lambda_)     \n",
    "        \n",
    "    def test(self, x, y, w, h):\n",
    "                \n",
    "        return { 'mse': compute_mse(y, x, w) }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
