{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from algebra import *\n",
    "from cache import *\n",
    "from costs import *\n",
    "from features import *\n",
    "from gradients import *\n",
    "from helpers import *\n",
    "from model import *\n",
    "from splits import *\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB_SAMPLE = True\n",
    "CACHE_DIR = \"test/cache/\" if SUB_SAMPLE else \"cache/\"\n",
    "SUBMISSIONS_DIR = \"test/submissions/\" if SUB_SAMPLE else \"submissions/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x, ids = load_csv_data('data/train.csv', SUB_SAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Analytical Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression with Fixed Degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Without Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_standardize_expand(y, x, h):\n",
    "        \n",
    "    degree = int(h['degree'])\n",
    "\n",
    "    x = remove_errors(x)\n",
    "    x = remove_outliers(x)\n",
    "    x = standardize_all(x)\n",
    "    x = remove_nan_features(x)\n",
    "    x = build_poly(x, degree)\n",
    "\n",
    "    return y, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_analytical(y, x, h):\n",
    "\n",
    "    lambda_ = float(h['lambda'])\n",
    "    degree = int(h['degree'])\n",
    "\n",
    "    w = ridge_regression(y, x, lambda_)\n",
    "    \n",
    "    return {\n",
    "        'w': w,\n",
    "        'mse': compute_mse(y, x, w)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = { \n",
    "    'degree': [5, 6, 7], \n",
    "    'lambda': 1e-4,\n",
    "}\n",
    "\n",
    "_ = evaluate(\n",
    "    clean = clean_standardize_expand, \n",
    "    fit   = ridge_regression_analytical, \n",
    "    x     = x, \n",
    "    y     = y, \n",
    "    hs    = hs, \n",
    "    cache = CACHE_DIR + 'clean_standardize_expand_ridge_regression_analytical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Cross-Validation\n",
    "\n",
    "Here, we implement the same model with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = { \n",
    "    'degree': np.arange(4, 16), \n",
    "    'lambda': np.logspace(-8, -4, 5),\n",
    "    'k_fold': 4,\n",
    "    'seed': 0\n",
    "}\n",
    "\n",
    "def mse(y, x, w):\n",
    "    return { 'mse' : compute_mse(y, x, w) }\n",
    "\n",
    "\n",
    "\n",
    "evaluate(\n",
    "    clean = cross_validate(ridge_regression_analytical, mse), \n",
    "    fit   = fit_function, \n",
    "    x     = x,\n",
    "    y     = y, \n",
    "    hs    = hs, \n",
    "    cache = CACHE_DIR + 'clean_standardize_expand_cross_validate_ridge_regression_analytical_mse'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myModel.predict(best_h, x, y, SUBMISSIONS_DIR + 'RidgeRegression_MSE_FixedDegree_CrossValidation_Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descents\n",
    "\n",
    "#### Least Square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_logistic(clean):\n",
    "    \n",
    "    def inner_function(y, x, h):\n",
    "        y, x = clean(y, x, h)\n",
    "        y = np.where(y == 1, 1, 0)\n",
    "        return y, x\n",
    "    \n",
    "    return inner_function\n",
    "\n",
    "def logistic_gradient(y, x, w, h):\n",
    "    \n",
    "    return compute_logistic_gradient(y, x, w)\n",
    "            \n",
    "def logistic_error(y, x, w, h):\n",
    "    \n",
    "    return { \n",
    "        'logistic_err': compute_logistic_error(y, x, w),\n",
    "        'n_err': compute_error_count(predict_logistic)(y, x, w)\n",
    "    }\n",
    "\n",
    "def logistic_gradient_ridge(y, x, w, h):\n",
    "    \n",
    "    lambda_ = h['lambda']\n",
    "    \n",
    "    return compute_logistic_gradient(y, x, w) + lambda_ * w\n",
    "\n",
    "def logistic_error_and_ridge(y, x, w, h):\n",
    "    \n",
    "    lambda_ = h['lambda']\n",
    "    \n",
    "    ridge_norm = np.linalg.norm(w, 2) * lambda_\n",
    "    logistic_err = compute_logistic_error(y, x, w)\n",
    "    n_err = compute_error_count(predict_logistic)(y, x, w)\n",
    "    \n",
    "    return {\n",
    "        'logistic_err': logistic_err,\n",
    "        'ridge_norm': ridge_norm,\n",
    "        'total_loss': logistic_err + ridge_norm,\n",
    "        'n_err': n_err\n",
    "    }\n",
    "\n",
    "def logistic_gradient_lasso(y, x, w, h):\n",
    "    \n",
    "    lambda_ = h['lambda']\n",
    "    \n",
    "    return compute_logistic_gradient(y, x, w) + lambda_ * np.sign(w)\n",
    "\n",
    "def logistic_error_and_lasso(y, x, w, h):\n",
    "    \n",
    "    lambda_ = h['lambda']\n",
    "    \n",
    "    lasso_norm = np.linalg.norm(w, 1) * lambda_\n",
    "    logistic_err = compute_logistic_error(y, x, w)\n",
    "    n_err = compute_error_count(predict_logistic)(y, x, w)\n",
    "    \n",
    "    return {\n",
    "        'logistic_err': logistic_err,\n",
    "        'lasso_norm': lasso_norm,\n",
    "        'total_loss': logistic_err + lasso_norm,\n",
    "        'n_err': n_err\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = {\n",
    "    'batch_size': 2500,\n",
    "    'degree': np.concatenate([[-2], np.arange(1, 7)]),\n",
    "    'gamma': [1e-2, 1e-3], \n",
    "    'k_fold': 4,\n",
    "    'lambda': 0,\n",
    "    'max_iters': 3000,\n",
    "    'num_batches': 1,\n",
    "    'seed': 1,\n",
    "}\n",
    "\n",
    "cache = Cache(CACHE_DIR + 'clean_standardize_expand_stochastic_logistic_descent')\n",
    "\n",
    "_ = evaluate(\n",
    "    clean = map_logistic(clean_standardize_expand), \n",
    "    fit   = descent_with_cache(\n",
    "        descent    = stochastic_gradient_descent_e(logistic_gradient), \n",
    "        loss       = logistic_error, \n",
    "        round_size = 100,\n",
    "        cache      = cache,\n",
    "        log        = True\n",
    "    ), \n",
    "    y     = y,\n",
    "    x     = x,\n",
    "    hs    = hs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Stochastic Gradient Descent With Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = {\n",
    "    'batch_size': 2500,\n",
    "    'degree': [-2] + np.arange(3, 4),\n",
    "    'gamma': [1e-2, 1e-3], \n",
    "    'lambda': [1e-2, 1e-3],\n",
    "    'k_fold': 4,\n",
    "    'max_iters': 1000,\n",
    "    'num_batches': 1,\n",
    "    'seed': 1,\n",
    "}\n",
    "\n",
    "cache = Cache(CACHE_DIR + 'clean_standardize_expand_stochastic_logistic_ridge_descent')\n",
    "\n",
    "_ = evaluate(\n",
    "    clean = map_logistic(clean_standardize_expand), \n",
    "    fit   = descent_with_cache(\n",
    "        descent    = stochastic_gradient_descent_e(logistic_gradient_ridge), \n",
    "        loss       = logistic_error_and_ridge, \n",
    "        round_size = 100,\n",
    "        cache      = cache,\n",
    "        log        = True\n",
    "    ), \n",
    "    y     = y,\n",
    "    x     = x,\n",
    "    hs    = hs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stochastic Gradient Descent With Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = {\n",
    "    'batch_size': 2500,\n",
    "    'degree': [-2, 1, 2, 3, 4, 5, 6],\n",
    "    'gamma': [1e-1, 1e-2, 1e-3], \n",
    "    'lambda': [1e-1, 1e-2, 1e-3],\n",
    "    'k_fold': 4,\n",
    "    'max_iters': 2000,\n",
    "    'num_batches': 1,\n",
    "    'seed': 0,\n",
    "}\n",
    "\n",
    "cache = Cache(CACHE_DIR + 'clean_standardize_expand_stochastic_logistic_lasso_descent')\n",
    "\n",
    "_ = evaluate(\n",
    "    clean = map_logistic(clean_standardize_expand), \n",
    "    fit   = descent_with_cache(\n",
    "        descent    = stochastic_gradient_descent_e(logistic_gradient_lasso), \n",
    "        loss       = logistic_error_and_lasso, \n",
    "        round_size = 100,\n",
    "        cache      = cache,\n",
    "        log        = True\n",
    "    ), \n",
    "    y     = y,\n",
    "    x     = x,\n",
    "    hs    = hs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 - {'batch_size': 2500, 'degree': -2, 'gamma': 0.01, 'k_fold': 4, 'lambda': 0, 'max_iters': 100, 'num_batches': 1, 'seed': 0, 'seed_cv': 0, 'avg_logistic_err_tr': 0.5148099735635681, 'avg_n_err_tr': 0.2680666666666667, 'avg_logistic_err_te': 0.5209256188036445, 'avg_n_err_te': 0.2718}\n",
      "iteration 200 - {'batch_size': 2500, 'degree': -2, 'gamma': 0.01, 'k_fold': 4, 'lambda': 0, 'max_iters': 100, 'num_batches': 1, 'seed': 100, 'seed_cv': 0, 'avg_logistic_err_tr': 0.48059641413096577, 'avg_n_err_tr': 0.2265333333333333, 'avg_logistic_err_te': 0.4907112787716249, 'avg_n_err_te': 0.2354}\n",
      "iteration 300 - {'batch_size': 2500, 'degree': -2, 'gamma': 0.01, 'k_fold': 4, 'lambda': 0, 'max_iters': 100, 'num_batches': 1, 'seed': 200, 'seed_cv': 0, 'avg_logistic_err_tr': 0.46079372334368063, 'avg_n_err_tr': 0.21573333333333333, 'avg_logistic_err_te': 0.4740086527943729, 'avg_n_err_te': 0.222}\n",
      "iteration 400 - {'batch_size': 2500, 'degree': -2, 'gamma': 0.01, 'k_fold': 4, 'lambda': 0, 'max_iters': 100, 'num_batches': 1, 'seed': 300, 'seed_cv': 0, 'avg_logistic_err_tr': 0.4476739939569579, 'avg_n_err_tr': 0.20299999999999999, 'avg_logistic_err_te': 0.4624770712523724, 'avg_n_err_te': 0.21359999999999998}\n"
     ]
    }
   ],
   "source": [
    "hs = {\n",
    "    'batch_size': 2500,\n",
    "    'degree': np.concatenate([[-2], np.arange(1, 7)]),\n",
    "    'gamma': [1e-2, 1e-3], \n",
    "    'k_fold': 4,\n",
    "    'lambda': 0,\n",
    "    'max_iters': 3000,\n",
    "    'num_batches': 1,\n",
    "    'seed': 0,\n",
    "    'seed_cv': 0\n",
    "}\n",
    "\n",
    "cache = Cache(CACHE_DIR + 'clean_standardize_expand_stochastic_logistic_descent_cross_validate')\n",
    "\n",
    "_ = evaluate(\n",
    "    clean = map_logistic(clean_standardize_expand), \n",
    "    fit   = descent_with_cache(\n",
    "        descent    = cross_validate_descent(\n",
    "            stochastic_gradient_descent_e(logistic_gradient), \n",
    "            logistic_error\n",
    "        ),\n",
    "        round_size = 100,\n",
    "        cache      = cache,\n",
    "        multiple   = True,\n",
    "        log        = True\n",
    "    ), \n",
    "    y     = y,\n",
    "    x     = x,\n",
    "    hs    = hs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'seed_cv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-f516c5e1bbac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0my\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mx\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mhs\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m )\n",
      "\u001b[0;32m~/Dropbox/Dev/EPFL/Machine Learning/Project 1/model.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(clean, fit, y, x, hs)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;31m# res = pool.starmap(self.execute, hs_params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclean_and_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhs_params\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_heatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Dev/EPFL/Machine Learning/Project 1/model.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;31m# res = pool.starmap(self.execute, hs_params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclean_and_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhs_params\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_heatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Dev/EPFL/Machine Learning/Project 1/model.py\u001b[0m in \u001b[0;36minner_function\u001b[0;34m(y, x, h)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Dev/EPFL/Machine Learning/Project 1/model.py\u001b[0m in \u001b[0;36minner_function\u001b[0;34m(y, x, h)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mmodified_h\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed_iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodified_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmultiple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Dev/EPFL/Machine Learning/Project 1/model.py\u001b[0m in \u001b[0;36minner_function\u001b[0;34m(y, x, h, initial_w)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mk_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'k_fold'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mseed_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seed_cv'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;31m# Split data in k fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'seed_cv'"
     ]
    }
   ],
   "source": [
    "hs = {\n",
    "    'batch_size': 2500,\n",
    "    'degree': 3,\n",
    "    'gamma': [1e-2, 1e-3], \n",
    "    'k_fold': 4,\n",
    "    'lambda': 0,\n",
    "    'max_iters': 1000,\n",
    "    'num_batches': 1,\n",
    "    'seed': 0,\n",
    "    'seed_cv': 0\n",
    "}\n",
    "\n",
    "cache = Cache(CACHE_DIR + 'clean_standardize_expand_stochastic_logistic_descent_cross_validate')\n",
    "\n",
    "def clean_expand(y, x, h):\n",
    "        \n",
    "    degree = int(h['degree'])\n",
    "\n",
    "    x = remove_errors(x)\n",
    "    x = remove_outliers(x)\n",
    "    x = remove_nan_features(x)\n",
    "    x = build_poly(x, degree)\n",
    "\n",
    "    return y, x\n",
    "\n",
    "_ = evaluate(\n",
    "    clean = map_logistic(clean_standardize_expand), \n",
    "    fit   = descent_with_cache(\n",
    "        descent    = cross_validate_descent(\n",
    "            stochastic_gradient_descent_e(logistic_gradient), \n",
    "            logistic_error\n",
    "        ),\n",
    "        round_size = 100,\n",
    "        cache      = cache,\n",
    "        multiple   = True,\n",
    "        log        = True\n",
    "    ), \n",
    "    y     = y,\n",
    "    x     = x,\n",
    "    hs    = hs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
