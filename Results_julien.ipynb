{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from algebra import *\n",
    "from cache import *\n",
    "from costs import *\n",
    "from features import *\n",
    "from gradients import *\n",
    "from helpers import *\n",
    "from model import *\n",
    "from splits import *\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB_SAMPLE = True\n",
    "CACHE_DIR = \"test/cache/\" if SUB_SAMPLE else \"cache/\"\n",
    "SUBMISSIONS_DIR = \"test/submissions/\" if SUB_SAMPLE else \"submissions/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x, ids = load_csv_data('data/train.csv', SUB_SAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Analytical Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression with Fixed Degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Without Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_standardize_expand(y, x, h):\n",
    "        \n",
    "    degree = int(h['degree'])\n",
    "\n",
    "    x = remove_errors(x)\n",
    "    x = remove_outliers(x)\n",
    "    x = standardize_all(x)\n",
    "    x = remove_nan_features(x)\n",
    "    x = build_poly(x, degree)\n",
    "\n",
    "    return y, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_analytical(y, x, h):\n",
    "\n",
    "    lambda_ = float(h['lambda'])\n",
    "    degree = int(h['degree'])\n",
    "\n",
    "    w = ridge_regression(y, x, lambda_)\n",
    "    \n",
    "    return {\n",
    "        'w': w,\n",
    "        'mse': compute_mse(y, x, w)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = { \n",
    "    'degree': [5, 6, 7], \n",
    "    'lambda': 1e-4,\n",
    "}\n",
    "\n",
    "_ = evaluate(\n",
    "    clean = clean_standardize_expand, \n",
    "    fit   = ridge_regression_analytical, \n",
    "    x     = x, \n",
    "    y     = y, \n",
    "    hs    = hs, \n",
    "    cache = CACHE_DIR + 'clean_standardize_expand_ridge_regression_analytical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Cross-Validation\n",
    "\n",
    "Here, we implement the same model with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = { \n",
    "    'degree': np.arange(4, 16), \n",
    "    'lambda': np.logspace(-8, -4, 5),\n",
    "    'k_fold': 4,\n",
    "    'seed': 0\n",
    "}\n",
    "\n",
    "def mse(y, x, w):\n",
    "    return { 'mse' : compute_mse(y, x, w) }\n",
    "\n",
    "\n",
    "\n",
    "evaluate(\n",
    "    clean = cross_validate(ridge_regression_analytical, mse), \n",
    "    fit   = fit_function, \n",
    "    x     = x,\n",
    "    y     = y, \n",
    "    hs    = hs, \n",
    "    cache = CACHE_DIR + 'clean_standardize_expand_cross_validate_ridge_regression_analytical_mse'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myModel.predict(best_h, x, y, SUBMISSIONS_DIR + 'RidgeRegression_MSE_FixedDegree_CrossValidation_Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descents\n",
    "\n",
    "#### Least Square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_logistic(clean):\n",
    "    \n",
    "    def inner_function(y, x, h):\n",
    "        y, x = clean(y, x, h)\n",
    "        y = np.where(y == 1, 1, 0)\n",
    "        return y, x\n",
    "    \n",
    "    return inner_function\n",
    "\n",
    "def logistic_gradient(y, x, w, h):\n",
    "    \n",
    "    return compute_logistic_gradient(y, x, w)\n",
    "            \n",
    "def logistic_error(y, x, w):\n",
    "    \n",
    "    return { \n",
    "        'logistic_err': compute_logistic_error(y, x, w),\n",
    "        'n_err': compute_error_count(y, x, w)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_error(y, x, w, h):\n",
    "    \n",
    "    lambda_ = h['lambda']\n",
    "    \n",
    "    logistic_err = compute_logistic_error(y, x, w)\n",
    "    n_err = compute_error_count(predict_logistic)(y, x, w)\n",
    "    \n",
    "    return {\n",
    "        'logistic_err': logistic_err,\n",
    "        'n_err': n_err\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = {\n",
    "    'batch_size': 2500,\n",
    "    'degree': np.concatenate([[-2], np.arange(1, 7)]),\n",
    "    'gamma': [1e-2, 1e-3], \n",
    "    'k_fold': 4,\n",
    "    'lambda': 0,\n",
    "    'max_iters': 3000,\n",
    "    'num_batches': 1,\n",
    "    'seed': 1,\n",
    "}\n",
    "\n",
    "cache = Cache(CACHE_DIR + 'clean_standardize_expand_stochastic_logistic_descent')\n",
    "\n",
    "_ = evaluate(\n",
    "    clean = map_logistic(clean_standardize_expand), \n",
    "    fit   = descent_with_cache(\n",
    "        descent    = stochastic_gradient_descent_e(logistic_gradient), \n",
    "        loss       = logistic_error, \n",
    "        round_size = 100,\n",
    "        cache      = cache,\n",
    "        log        = True\n",
    "    ), \n",
    "    y     = y,\n",
    "    x     = x,\n",
    "    hs    = hs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Stochastic Gradient Descent With Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_gradient_ridge(y, x, w, h):\n",
    "    \n",
    "    lambda_ = h['lambda']\n",
    "    \n",
    "    return compute_logistic_gradient(y, x, w) + lambda_ * w\n",
    "\n",
    "def logistic_error_and_ridge(y, x, w, h):\n",
    "    \n",
    "    lambda_ = h['lambda']\n",
    "    \n",
    "    ridge_norm = np.linalg.norm(w, 2) * lambda_\n",
    "    logistic_err = compute_logistic_error(y, x, w)\n",
    "    n_err = compute_error_count(predict_logistic)(y, x, w)\n",
    "    \n",
    "    return {\n",
    "        'logistic_err': logistic_err,\n",
    "        'ridge_norm': ridge_norm,\n",
    "        'total_loss': logistic_err + ridge_norm,\n",
    "        'n_err': n_err\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = {\n",
    "    'batch_size': 2500,\n",
    "    'degree': [-2] + np.arange(3, 4),\n",
    "    'gamma': [1e-2, 1e-3], \n",
    "    'lambda': [1e-2, 1e-3],\n",
    "    'k_fold': 4,\n",
    "    'max_iters': 1000,\n",
    "    'num_batches': 1,\n",
    "    'seed': 1,\n",
    "}\n",
    "\n",
    "cache = Cache(CACHE_DIR + 'clean_standardize_expand_stochastic_logistic_ridge_descent')\n",
    "\n",
    "_ = evaluate(\n",
    "    clean = map_logistic(clean_standardize_expand), \n",
    "    fit   = descent_with_cache(\n",
    "        descent    = stochastic_gradient_descent_e(logistic_gradient_ridge), \n",
    "        loss       = logistic_error_and_ridge, \n",
    "        round_size = 100,\n",
    "        cache      = cache,\n",
    "        log        = True\n",
    "    ), \n",
    "    y     = y,\n",
    "    x     = x,\n",
    "    hs    = hs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stochastic Gradient Descent With Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_gradient_lasso(y, x, w, h):\n",
    "    \n",
    "    lambda_ = h['lambda']\n",
    "    \n",
    "    return compute_logistic_gradient(y, x, w) + lambda_ * np.sign(w)\n",
    "\n",
    "def logistic_error_and_lasso(y, x, w, h):\n",
    "    \n",
    "    lambda_ = h['lambda']\n",
    "    \n",
    "    lasso_norm = np.linalg.norm(w, 1) * lambda_\n",
    "    logistic_err = compute_logistic_error(y, x, w)\n",
    "    n_err = compute_error_count(predict_logistic)(y, x, w)\n",
    "    \n",
    "    return {\n",
    "        'logistic_err': logistic_err,\n",
    "        'lasso_norm': lasso_norm,\n",
    "        'total_loss': logistic_err + lasso_norm,\n",
    "        'n_err': n_err\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = {\n",
    "    'batch_size': 2500,\n",
    "    'degree': [-2, 1, 2, 3, 4, 5, 6],\n",
    "    'gamma': [1e-1, 1e-2, 1e-3], \n",
    "    'lambda': [1e-1, 1e-2, 1e-3],\n",
    "    'k_fold': 4,\n",
    "    'max_iters': 2000,\n",
    "    'num_batches': 1,\n",
    "    'seed': 0,\n",
    "}\n",
    "\n",
    "cache = Cache(CACHE_DIR + 'clean_standardize_expand_stochastic_logistic_lasso_descent')\n",
    "\n",
    "_ = evaluate(\n",
    "    clean = map_logistic(clean_standardize_expand), \n",
    "    fit   = descent_with_cache(\n",
    "        descent    = stochastic_gradient_descent_e(logistic_gradient_lasso), \n",
    "        loss       = logistic_error_and_lasso, \n",
    "        round_size = 100,\n",
    "        cache      = cache,\n",
    "        log        = True\n",
    "    ), \n",
    "    y     = y,\n",
    "    x     = x,\n",
    "    hs    = hs\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
